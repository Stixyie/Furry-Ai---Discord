import os
import json
import re
import numpy as np
from typing import Dict, List, Any, Optional, Union
from datetime import datetime, timedelta
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from scipy.stats import entropy
from sklearn.metrics.pairwise import cosine_similarity
import discord
from discord.ext import commands
import logging

logger = logging.getLogger(__name__)

class TurkishTextNormalizer:
    @staticmethod
    def normalize_text(text: str) -> str:
        """Metni normalize et"""
        try:
            # GiriÅŸ kontrolÃ¼
            if not text or not isinstance(text, str):
                logger.warning("GeÃ§ersiz metin giriÅŸi: Metin boÅŸ veya geÃ§ersiz.")
                return ""
            
            # KÃ¼Ã§Ã¼k harfe Ã§evir
            text = text.lower()
            
            # TÃ¼rkÃ§e karakterleri dÃ¼zelt
            text = text.replace('Ä±', 'i').replace('Ä°', 'i')
            text = text.replace('ÄŸ', 'g').replace('Ä', 'g')
            text = text.replace('ÅŸ', 's').replace('Å', 's')
            
            # Noktalama iÅŸaretlerini temizle
            text = re.sub(r'[^\w\s]', '', text)
            
            # Gereksiz boÅŸluklarÄ± temizle
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text
        except Exception as e:
            logger.error(f"Metin normalize etme hatasÄ±: {e}")
            return ""
    
    @staticmethod
    def remove_stopwords(text: str) -> str:
        """TÃ¼rkÃ§e stop words'leri Ã§Ä±kar"""
        try:
            # GiriÅŸ kontrolÃ¼
            if not text or not isinstance(text, str):
                logger.warning("GeÃ§ersiz metin giriÅŸi: Metin boÅŸ veya geÃ§ersiz.")
                return ""
            
            turkish_stopwords = set([
                'a', 'an', 've', 'veya', 'ama', 'ancak', 'fakat', 
                'ki', 'de', 'da', 'mi', 'mu', 'mÃ¼', 'mÄ±', 
                'deÄŸil', 'gibi', 'iÃ§in', 'kadar', 'Ã§ok', 'az',
                'ben', 'sen', 'o', 'biz', 'siz', 'onlar',
                'bir', 'bu', 'ÅŸu', 'her', 'bazÄ±', 'tÃ¼m'
            ])
            
            # Metni kelimelere ayÄ±r ve stop words'leri Ã§Ä±kar
            words = text.split()
            filtered_words = [word for word in words if word not in turkish_stopwords]
            
            return ' '.join(filtered_words)
        except Exception as e:
            logger.error(f"Stop words Ã§Ä±karma hatasÄ±: {e}")
            return text  # Hata durumunda orijinal metni dÃ¶ndÃ¼r

class AdvancedMemorySystem:
    def __init__(
        self, 
        memory_dir: str = None, 
        max_memory_size: int = 10000,
        memory_decay_rate: float = 0.1
    ):
        # Temel yapÄ±landÄ±rma
        self.memory_dir = memory_dir or os.path.join(
            os.path.dirname(__file__), 
            '..', 
            'data', 
            'advanced_memories'
        )
        os.makedirs(self.memory_dir, exist_ok=True)
        
        # Metin normalleÅŸtirici
        self.text_normalizer = TurkishTextNormalizer()
        
        # GeliÅŸmiÅŸ yapÄ±landÄ±rma parametreleri
        self.MAX_MEMORY_SIZE = max_memory_size
        self.MEMORY_DECAY_RATE = memory_decay_rate
        
        # TF-IDF Vectorizer
        self.vectorizer = TfidfVectorizer(
            preprocessor=self.text_normalizer.normalize_text,
            stop_words=None  # Manuel stop words kontrolÃ¼
        )
        
        # Bellek katmanlarÄ±
        self.memory_layers = {
            'semantic': [],
            'contextual': [],
            'temporal': []
        }
        
        # Bellek analiz araÃ§larÄ±
        self.memory_stats = {
            'total_memories': 0,
            'memory_entropy': 0.0,
            'semantic_diversity': 0.0
        }
        
        # Bellek yÃ¼kleme ve baÅŸlatma
        self._load_memory_layers()
        self._initialize_memory_analysis()
    
    def _load_memory_layers(self):
        """TÃ¼m bellek katmanlarÄ±nÄ± yÃ¼kle"""
        for layer, memories in self.memory_layers.items():
            layer_file = os.path.join(self.memory_dir, f'{layer}_memories.json')
            
            if os.path.exists(layer_file):
                with open(layer_file, 'r', encoding='utf-8') as f:
                    layer_memories = json.load(f)
                    memories.extend(layer_memories)
    
    def add_memory(
        self, 
        text: str, 
        context: Optional[Dict[str, Any]] = None,
        layer: str = 'semantic',
        importance: float = 0.5
    ):
        """GeliÅŸmiÅŸ bellek ekleme"""
        try:
            # GiriÅŸ kontrolÃ¼
            if not text or not isinstance(text, str):
                logger.warning("GeÃ§ersiz metin giriÅŸi: Metin boÅŸ veya geÃ§ersiz.")
                return {
                    'original_text': text,
                    'normalized_text': '',
                    'filtered_text': '',
                    'success': False
                }
            
            # VarsayÄ±lan context
            context = context or {}
            
            # Metni normalize et
            normalized_text = self.text_normalizer.normalize_text(text)
            filtered_text = self.text_normalizer.remove_stopwords(normalized_text)
            
            # Katman kontrolÃ¼
            if layer not in self.memory_layers:
                logger.warning(f"GeÃ§ersiz bellek katmanÄ±: {layer}. VarsayÄ±lan 'semantic' katmanÄ± kullanÄ±lacak.")
                layer = 'semantic'
            
            # Bellek Ã¶ÄŸesi oluÅŸtur
            memory_entry = {
                'text': text,
                'normalized_text': normalized_text,
                'filtered_text': filtered_text,
                'context': context,
                'layer': layer,
                'importance': importance,
                'timestamp': datetime.now().isoformat()
            }
            
            # BelleÄŸe ekle
            self.memory_layers[layer].append(memory_entry)
            
            # Bellek boyutunu yÃ¶net
            self._manage_memory_size()
            
            # Ä°statistikleri gÃ¼ncelle
            self._update_memory_stats()
            
            # BaÅŸarÄ±lÄ± dÃ¶nÃ¼ÅŸ
            return {
                'original_text': text,
                'normalized_text': normalized_text,
                'filtered_text': filtered_text,
                'success': True
            }
        
        except Exception as e:
            logger.error(f"Bellek ekleme hatasÄ±: {e}")
            return {
                'original_text': text,
                'normalized_text': '',
                'filtered_text': '',
                'success': False
            }
    
    def _manage_memory_size(self):
        """Bellek boyutunu ve Ã§eÅŸitliliÄŸini yÃ¶net"""
        # Toplam bellek boyutu kontrolÃ¼
        total_memories = sum(len(layer) for layer in self.memory_layers.values())
        
        if total_memories > self.MAX_MEMORY_SIZE:
            # En az Ã¶nemli bellek giriÅŸlerini Ã§Ä±kar
            self._remove_least_important_memories()
    
    def _remove_least_important_memories(self):
        """En az Ã¶nemli bellek giriÅŸlerini Ã§Ä±kar"""
        for layer, memories in self.memory_layers.items():
            # Ã–nem ve zamana gÃ¶re sÄ±rala
            sorted_memories = sorted(
                memories, 
                key=lambda m: (m['importance'], datetime.fromisoformat(m['timestamp']))
            )
            
            # Belirli sayÄ±da en az Ã¶nemli belleÄŸi Ã§Ä±kar
            memories[:] = sorted_memories[len(sorted_memories) // 4:]
    
    def _update_memory_stats(self):
        """Bellek istatistiklerini gÃ¼ncelle"""
        # Toplam bellek sayÄ±sÄ±
        self.memory_stats['total_memories'] = sum(
            len(layer) for layer in self.memory_layers.values()
        )
        
        # Bellek entropisi
        all_texts = [
            memory['normalized_text'] 
            for layer in self.memory_layers.values() 
            for memory in layer
        ]
        
        if all_texts:
            # TF-IDF matrisinden entropi hesaplama
            tfidf_matrix = self.vectorizer.fit_transform(all_texts)
            
            # Entropi hesaplama
            feature_entropies = [
                entropy(tfidf_matrix[:, i].toarray().flatten()) 
                for i in range(tfidf_matrix.shape[1])
            ]
            
            self.memory_stats['memory_entropy'] = np.mean(feature_entropies)
    
    def _get_layer_embeddings(
        self, 
        memories: List[Dict[str, Any]], 
        layer: str
    ) -> np.ndarray:
        """Belirli bir katman iÃ§in embedding oluÅŸtur"""
        # EÄŸer hafÄ±zada hiÃ§ Ã¶ÄŸe yoksa, boÅŸ bir numpy array dÃ¶ndÃ¼r
        if not memories:
            return np.array([])
        
        texts = [memory['normalized_text'] for memory in memories]
        
        # EÄŸer tÃ¼m textler boÅŸsa, boÅŸ bir numpy array dÃ¶ndÃ¼r
        if not any(texts):
            return np.array([])
        
        try:
            # TF-IDF embedding
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            
            if layer == 'temporal':
                # Zamansal Ã¶zellikler eklenmiÅŸ embedding
                temporal_features = np.array([
                    [
                        (datetime.now() - datetime.fromisoformat(memory['timestamp'])).total_seconds() / (24 * 3600),
                        memory.get('importance', 0.5)
                    ] 
                    for memory in memories
                ])
                
                return np.hstack([tfidf_matrix.toarray(), temporal_features])
            
            return tfidf_matrix.toarray()
        except ValueError:
            # EÄŸer TF-IDF oluÅŸturulamazsa, boÅŸ bir numpy array dÃ¶ndÃ¼r
            return np.array([])
    
    def semantic_search(
        self, 
        query: str, 
        layer: str = 'all', 
        top_k: int = 5, 
        min_score: float = 0.7
    ) -> List[Dict[str, Any]]:
        """GeliÅŸmiÅŸ semantik arama"""
        # Sorguyu normalize et
        normalized_query = self.text_normalizer.normalize_text(query)
        filtered_query = self.text_normalizer.remove_stopwords(normalized_query)
        
        # Aranacak katmanlarÄ± belirle
        search_layers = [layer] if layer != 'all' else list(self.memory_layers.keys())
        
        results = []
        
        for search_layer in search_layers:
            memories = self.memory_layers[search_layer]
            
            # Embedding oluÅŸtur
            layer_embeddings = self._get_layer_embeddings(memories, search_layer)
            
            # EÄŸer embedding oluÅŸturulamazsa, sonraki katmana geÃ§
            if layer_embeddings.size == 0:
                continue
            
            # Query embedding
            query_embedding = self.vectorizer.transform([normalized_query]).toarray()
            
            # Benzerlik hesaplama
            similarities = cosine_similarity(query_embedding, layer_embeddings)[0]
            
            # En benzer bellek giriÅŸlerini bul
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            for idx in top_indices:
                similarity_score = similarities[idx]
                
                if similarity_score >= min_score:
                    result = {
                        'text': memories[idx]['original_text'],
                        'layer': search_layer,
                        'context': memories[idx].get('context', {}),
                        'similarity_score': similarity_score
                    }
                    results.append(result)
        
        return sorted(results, key=lambda x: x['similarity_score'], reverse=True)
    
    def analyze_memory_clusters(self) -> Dict[str, Any]:
        """Bellek kÃ¼melerini analiz et"""
        all_embeddings = []
        cluster_labels = []
        
        # TÃ¼m katmanlardan embedding topla
        for layer, memories in self.memory_layers.items():
            layer_embeddings = self._get_layer_embeddings(memories, layer)
            all_embeddings.append(layer_embeddings)
            cluster_labels.extend([layer] * len(layer_embeddings))
        
        all_embeddings = np.concatenate(all_embeddings)
        
        # StandartlaÅŸtÄ±rma
        scaler = StandardScaler()
        scaled_embeddings = scaler.fit_transform(all_embeddings)
        
        # K-means kÃ¼meleme
        kmeans = KMeans(n_clusters=5, random_state=42)
        kmeans.fit(scaled_embeddings)
        
        return {
            'cluster_centers': kmeans.cluster_centers_.tolist(),
            'cluster_labels': kmeans.labels_.tolist(),
            'memory_stats': self.memory_stats
        }
    
    def save_memories(self):
        """TÃ¼m bellek katmanlarÄ±nÄ± kaydet"""
        for layer, memories in self.memory_layers.items():
            layer_file = os.path.join(self.memory_dir, f'{layer}_memories.json')
            
            with open(layer_file, 'w', encoding='utf-8') as f:
                json.dump(memories, f, ensure_ascii=False, indent=4)
    
    def clear_memories(self, layer: str = 'all'):
        """Bellek katmanlarÄ±nÄ± temizle"""
        if layer == 'all':
            for layer_name in self.memory_layers:
                self.memory_layers[layer_name].clear()
        else:
            self.memory_layers[layer].clear()
        
        # Ä°statistikleri sÄ±fÄ±rla
        self._initialize_memory_analysis()
    
    def _initialize_memory_analysis(self):
        """Bellek analiz parametrelerini baÅŸlat"""
        self.memory_stats = {
            'total_memories': 0,
            'memory_entropy': 0.0,
            'semantic_diversity': 0.0
        }

class MemorySystem:
    def __init__(self, max_memory_size: int = 1000):
        """GeliÅŸmiÅŸ bellek sistemi baÅŸlatÄ±cÄ±sÄ±"""
        # Text normalizer
        self.text_normalizer = TurkishTextNormalizer()
        
        # Bellek katmanlarÄ±
        self.memory_layers = {
            'semantic': [],     # Anlamsal bellek
            'contextual': [],   # BaÄŸlamsal bellek
            'conversation': [], # KonuÅŸma belleÄŸi
            'knowledge': []     # Bilgi belleÄŸi
        }
        
        # Maksimum bellek boyutu
        self.max_memory_size = max_memory_size
        
        # Bellek istatistikleri
        self.memory_stats = {
            'total_memories': 0,
            'layer_distribution': {layer: 0 for layer in self.memory_layers.keys()}
        }

    def add_memory(
        self, 
        text: str, 
        context: Optional[Dict[str, Any]] = None,
        layer: str = 'semantic',
        importance: float = 0.5
    ) -> Dict[str, Any]:
        """GeliÅŸmiÅŸ bellek ekleme"""
        try:
            # GiriÅŸ kontrolÃ¼
            if not text or not isinstance(text, str):
                logger.warning("GeÃ§ersiz bellek giriÅŸi: Metin boÅŸ veya geÃ§ersiz.")
                text = "VarsayÄ±lan bellek giriÅŸi"
            
            # Metni normalize et
            normalized_text = self.text_normalizer.normalize_text(text)
            
            # Stop words'leri Ã§Ä±kar
            filtered_text = self.text_normalizer.remove_stopwords(normalized_text)
            
            # Katman kontrolÃ¼
            if layer not in self.memory_layers:
                logger.warning(f"GeÃ§ersiz bellek katmanÄ±: {layer}. VarsayÄ±lan 'semantic' katmanÄ± kullanÄ±lacak.")
                layer = 'semantic'
            
            # Bellek giriÅŸi oluÅŸtur
            memory_entry = {
                'text': text,  # Orijinal metin
                'original_text': text,
                'normalized_text': normalized_text,
                'filtered_text': filtered_text,
                'context': context or {},
                'timestamp': datetime.now().isoformat(),
                'importance': importance,
                'layer': layer
            }
            
            # Katmana ekle
            self.memory_layers[layer].append(memory_entry)
            
            # Bellek boyutu kontrolÃ¼
            self._manage_memory_size()
            
            # Bellek istatistiklerini gÃ¼ncelle
            self._update_memory_stats()
            
            logger.info(f"Bellek Ã¶ÄŸesi baÅŸarÄ±yla eklendi: {layer} katmanÄ±")
            
            return memory_entry
        
        except Exception as e:
            logger.error(f"Bellek ekleme hatasÄ±: {e}")
            
            # Hata durumunda varsayÄ±lan bir bellek Ã¶ÄŸesi ekle
            fallback_memory = {
                'text': text or 'VarsayÄ±lan bellek Ã¶ÄŸesi',
                'original_text': text or 'VarsayÄ±lan bellek Ã¶ÄŸesi',
                'normalized_text': 'varsayilan bellek ogesi',
                'filtered_text': 'varsayilan bellek ogesi',
                'context': {'type': 'fallback', 'error': str(e)},
                'layer': 'semantic',
                'importance': 0.1,
                'timestamp': datetime.now().isoformat()
            }
            
            self.memory_layers['semantic'].append(fallback_memory)
            
            return fallback_memory

    def _manage_memory_size(self):
        """Bellek boyutunu kontrol et ve yÃ¶net"""
        try:
            # TÃ¼m katmanlar iÃ§in toplam bellek boyutu kontrolÃ¼
            total_memories = sum(len(layer) for layer in self.memory_layers.values())
            
            if total_memories > self.max_memory_size:
                # En az Ã¶nemli bellek Ã¶ÄŸelerini sil
                for layer in self.memory_layers.values():
                    layer.sort(key=lambda x: x.get('importance', 0))
                    while len(layer) > self.max_memory_size // len(self.memory_layers):
                        removed_memory = layer.pop(0)
                        logger.info(f"Bellek boyutu aÅŸÄ±ldÄ±. DÃ¼ÅŸÃ¼k Ã¶nemli bellek Ã¶ÄŸesi silindi: {removed_memory['text'][:50]}...")
        
        except Exception as e:
            logger.error(f"Bellek boyutu yÃ¶netimi hatasÄ±: {e}")

    def _update_memory_stats(self):
        """Bellek istatistiklerini gÃ¼ncelle"""
        try:
            # Toplam bellek sayÄ±sÄ±nÄ± gÃ¼ncelle
            self.memory_stats['total_memories'] = sum(len(layer) for layer in self.memory_layers.values())
            
            # Katman daÄŸÄ±lÄ±mÄ±nÄ± gÃ¼ncelle
            for layer, memories in self.memory_layers.items():
                self.memory_stats['layer_distribution'][layer] = len(memories)
            
            # Semantik Ã§eÅŸitliliÄŸi hesapla
            unique_texts = set(memory['filtered_text'] for layer in self.memory_layers.values() for memory in layer)
            total_memories = self.memory_stats['total_memories']
            
            self.memory_stats['semantic_diversity'] = len(unique_texts) / total_memories if total_memories > 0 else 0.0
        
        except Exception as e:
            logger.error(f"Bellek istatistikleri gÃ¼ncelleme hatasÄ±: {e}")
            
            # Hata durumunda varsayÄ±lan deÄŸerler
            self.memory_stats = {
                'total_memories': 0,
                'layer_distribution': {layer: 0 for layer in self.memory_layers.keys()},
                'semantic_diversity': 0.0
            }

    def search_memory(
        self, 
        query: str, 
        layer: Optional[str] = None, 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """GeliÅŸmiÅŸ bellek arama"""
        try:
            # Sorguyu normalize et
            normalized_query = self.text_normalizer.normalize_text(query)
            filtered_query = self.text_normalizer.remove_stopwords(normalized_query)
            
            # Arama yapÄ±lacak katmanlarÄ± belirle
            search_layers = [layer] if layer else list(self.memory_layers.keys())
            
            # Benzerlik sonuÃ§larÄ±
            results = []
            
            for current_layer in search_layers:
                layer_memories = self.memory_layers.get(current_layer, [])
                
                for memory in layer_memories:
                    # Benzerlik hesapla
                    similarity_score = self._calculate_similarity(
                        filtered_query, 
                        memory.get('filtered_text', '')
                    )
                    
                    results.append({
                        'text': memory['text'],
                        'layer': current_layer,
                        'similarity_score': similarity_score,
                        'context': memory.get('context', {})
                    })
            
            # En yÃ¼ksek benzerlik skoruna gÃ¶re sÄ±rala
            return sorted(results, key=lambda x: x['similarity_score'], reverse=True)[:top_k]
        
        except Exception as e:
            logger.error(f"Bellek arama hatasÄ±: {e}")
            return []

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Metinler arasÄ±ndaki benzerliÄŸi hesapla"""
        try:
            # Metinleri vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼r
            vectorizer = TfidfVectorizer().fit_transform([text1, text2])
            
            # KosinÃ¼s benzerliÄŸini hesapla
            similarity = cosine_similarity(vectorizer[0:1], vectorizer[1:2])[0][0]
            
            return similarity
        
        except Exception as e:
            logger.error(f"Benzerlik hesaplama hatasÄ±: {e}")
            return 0.0

async def setup(bot):
    """Discord bot extension iÃ§in setup fonksiyonu"""
    bot.memory_system = AdvancedMemorySystem()
    print("Memory System extension yÃ¼klendi! ğŸ§ âœ¨")
